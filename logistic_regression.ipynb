{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.trainX = None\n",
    "        self.trainY = None\n",
    "        self.testX = None\n",
    "        self.testY = None\n",
    "        self.b = None                    \n",
    "    \n",
    "    def load_iris(self, multinomial=False):\n",
    "        \"\"\"\n",
    "        Load Iris data.\n",
    "        \n",
    "        If multinomial is false than all but 2 classes will be discarded\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv('iris.txt', names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class'])\n",
    "        self.df['class'] = self.df['class'].astype('category').cat.codes\n",
    "        for col in ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']:\n",
    "            self.df[col] = (self.df[col] - self.df[col].mean())/self.df[col].std()\n",
    "\n",
    "        self.X = self.df.to_numpy()[:,:4]\n",
    "        self.y = self.df.to_numpy()[:,4]\n",
    "        if multinomial==False:\n",
    "            self.X = self.X[self.y <= 1,:]\n",
    "            self.y = self.y[self.y <= 1]\n",
    "    \n",
    "    def split_data(self, p = 0.8):        \n",
    "        \"\"\"\n",
    "        Randomly split data into test and train\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self.X is not None, \"Need to load data before splitting\"\n",
    "        \n",
    "        mask = np.random.rand(self.X.shape[0]) < p\n",
    "           \n",
    "        \n",
    "        self.trainX = self.X[mask==True]\n",
    "        self.testX = self.X[mask==False]\n",
    "        \n",
    "        self.trainY = self.y[mask==True]\n",
    "        self.testY = self.y[mask==False]        \n",
    "        \n",
    "    def fit(self, lr=0.01):\n",
    "        \"\"\"\n",
    "        Fit data uses logistic regression\n",
    "        \"\"\"\n",
    "        \n",
    "        self.b = np.random.normal(0,1,size=(self.X.shape[1] + 1, 1))\n",
    "        X = np.hstack((np.ones((self.trainX.shape[0], 1)), self.trainX))\n",
    "        y = self.trainY.reshape(-1, 1)\n",
    "        \n",
    "        X_test = np.hstack((np.ones((self.testX.shape[0], 1)), self.testX))\n",
    "        y_test = self.testY.reshape(-1, 1)\n",
    "        \n",
    "        for i in range(20):                        \n",
    "            grad = -np.matmul(X.T, y - self.predict(X))\n",
    "            self.b = self.b - lr*grad            \n",
    "            \n",
    "            y_pred = self.predict(X_test)\n",
    "            y_pred_train = self.predict(X)\n",
    "            print(\"Train Loss: {:.3f}, Test Loss: {:.3f}\".format(self.loss(y, y_pred_train), self.loss(y_test, y_pred)))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for given data X\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-X@self.b))\n",
    "    \n",
    "    def loss(self, yTrue, yPred):\n",
    "        \"\"\"\n",
    "        Return cross entropy loss for true and predicted y values\n",
    "        \"\"\"\n",
    "        return -1*(np.sum(yTrue*np.log(yPred) + (1-yTrue)*np.log(1-yPred)))/len(yTrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.513, Test Loss: 0.406\n",
      "Train Loss: 0.402, Test Loss: 0.328\n",
      "Train Loss: 0.322, Test Loss: 0.272\n",
      "Train Loss: 0.265, Test Loss: 0.232\n",
      "Train Loss: 0.224, Test Loss: 0.202\n",
      "Train Loss: 0.193, Test Loss: 0.179\n",
      "Train Loss: 0.169, Test Loss: 0.161\n",
      "Train Loss: 0.151, Test Loss: 0.147\n",
      "Train Loss: 0.136, Test Loss: 0.135\n",
      "Train Loss: 0.124, Test Loss: 0.126\n",
      "Train Loss: 0.114, Test Loss: 0.117\n",
      "Train Loss: 0.106, Test Loss: 0.111\n",
      "Train Loss: 0.098, Test Loss: 0.105\n",
      "Train Loss: 0.092, Test Loss: 0.099\n",
      "Train Loss: 0.087, Test Loss: 0.095\n",
      "Train Loss: 0.082, Test Loss: 0.091\n",
      "Train Loss: 0.078, Test Loss: 0.087\n",
      "Train Loss: 0.074, Test Loss: 0.083\n",
      "Train Loss: 0.070, Test Loss: 0.080\n",
      "Train Loss: 0.067, Test Loss: 0.078\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.load_iris()\n",
    "lr.split_data(p=0.6)\n",
    "lr.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
